{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "InstantiationException",
     "evalue": "Error in call to target 'lib.optimizer.sketch.SketchOptimizer':\nRuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n')\nfull_key: model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:92\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_target_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/sketch2shape/lib/optimizer/sketch.py:15\u001b[0m, in \u001b[0;36mSketchOptimizer.__init__\u001b[0;34m(self, loss_mode, loss_weight, silhouette_loss, silhouette_weight, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      9\u001b[0m     loss_mode: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     14\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m loss_mode\n",
      "File \u001b[0;32m~/sketch2shape/lib/optimizer/latent.py:55\u001b[0m, in \u001b[0;36mLatentOptimizer.__init__\u001b[0;34m(self, data_dir, loss_ckpt_path, deepsdf_ckpt_path, optimizer, scheduler, latent_init, sketch_mode, retrieval_mode, reg_loss, reg_weight, prior_obj_id, prior_view_id, retrieval_k, mesh_resolution, mesh_chunk_size, n_render_steps, clamp_sdf, step_scale, surface_eps, sphere_eps, normal_eps, log_images, capture_rate, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# init deepsdf\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepsdf \u001b[38;5;241m=\u001b[39m \u001b[43mDeepSDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepsdf_ckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# mesh settings\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmesh_resolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh_resolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmesh_chunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmesh_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# rendering settings\u001b[39;49;00m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_render_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_render_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclamp_sdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclamp_sdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43msurface_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurface_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43msphere_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msphere_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormal_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormal_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepsdf\u001b[38;5;241m.\u001b[39mfreeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03mpassed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:63\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 63\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mpl_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# convert legacy checkpoints to the new format\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/lightning/fabric/utilities/cloud_io.py:56\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mopen(path_or_url, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/pickle.py:1254\u001b[0m, in \u001b[0;36m_Unpickler.load_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1253\u001b[0m pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m-> 1254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1382\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1381\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1382\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1383\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1384\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:1316\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1316\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:391\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 391\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/torch/_utils.py:115\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     untyped_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     untyped_storage\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInstantiationException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m cfg\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlatent_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 46\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstantiate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# transforms\u001b[39;00m\n\u001b[1;32m     49\u001b[0m transforms \u001b[38;5;241m=\u001b[39m [v2\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)), ToSketch(), DilateSketch(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:226\u001b[0m, in \u001b[0;36minstantiate\u001b[0;34m(config, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m     _convert_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mCONVERT, ConvertMode\u001b[38;5;241m.\u001b[39mNONE)\n\u001b[1;32m    224\u001b[0m     _partial_ \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpop(_Keys\u001b[38;5;241m.\u001b[39mPARTIAL, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstantiate_node\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_recursive_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_convert_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_partial_\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m OmegaConf\u001b[38;5;241m.\u001b[39mis_list(config):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Finalize config (convert targets to strings, merge with kwargs)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     config_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:347\u001b[0m, in \u001b[0;36minstantiate_node\u001b[0;34m(node, convert, recursive, partial, *args)\u001b[0m\n\u001b[1;32m    342\u001b[0m                 value \u001b[38;5;241m=\u001b[39m instantiate_node(\n\u001b[1;32m    343\u001b[0m                     value, convert\u001b[38;5;241m=\u001b[39mconvert, recursive\u001b[38;5;241m=\u001b[39mrecursive\n\u001b[1;32m    344\u001b[0m                 )\n\u001b[1;32m    345\u001b[0m             kwargs[key] \u001b[38;5;241m=\u001b[39m _convert_node(value, convert)\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_target_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# If ALL or PARTIAL non structured or OBJECT non structured,\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;66;03m# instantiate in dict and resolve interpolations eagerly.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;241m==\u001b[39m ConvertMode\u001b[38;5;241m.\u001b[39mALL \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    352\u001b[0m         convert \u001b[38;5;129;01min\u001b[39;00m (ConvertMode\u001b[38;5;241m.\u001b[39mPARTIAL, ConvertMode\u001b[38;5;241m.\u001b[39mOBJECT)\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m node\u001b[38;5;241m.\u001b[39m_metadata\u001b[38;5;241m.\u001b[39mobject_type \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    354\u001b[0m     ):\n",
      "File \u001b[0;32m~/miniconda3/envs/sketch2shape/lib/python3.11/site-packages/hydra/_internal/instantiate/_instantiate2.py:97\u001b[0m, in \u001b[0;36m_call_target\u001b[0;34m(_target_, _partial_, args, kwargs, full_key)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_key:\n\u001b[1;32m     96\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfull_key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InstantiationException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mInstantiationException\u001b[0m: Error in call to target 'lib.optimizer.sketch.SketchOptimizer':\nRuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n')\nfull_key: model"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lib.data.metainfo import MetaInfo\n",
    "from lib.data.transforms import BaseTransform\n",
    "import hydra\n",
    "from lib.utils.config import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from lib.data.transforms import BaseTransform, DilateSketch, ToSketch\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "def plot_images_np(images, size: int = 4):\n",
    "    if isinstance(images, list):\n",
    "        _, axes = plt.subplots(1, len(images), figsize=(size, size))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(size, size))\n",
    "        plt.imshow(images)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_images(images, size: int = 4):\n",
    "    if isinstance(images, list):\n",
    "        _, axes = plt.subplots(1, len(images), figsize=(size, size))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image.detach().cpu().numpy())\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(size, size))\n",
    "        plt.imshow(images.detach().cpu().numpy())\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "cfg = load_config(\"optimize_sketch\", [\"+dataset=shapenet_chair_4096\"])\n",
    "metainfo = MetaInfo(cfg.data.data_dir)\n",
    "\n",
    "cfg.model.prior_obj_id = metainfo.obj_ids[13]\n",
    "cfg.model.loss_ckpt_path = \"/home/borth/sketch2shape/checkpoints/latent_siamese_sketch_grayscale_latent_256.ckpt\"\n",
    "cfg.model.latent_init = \"latent\"\n",
    "\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "\n",
    "# transforms\n",
    "transforms = [v2.Resize((256, 256)), ToSketch(), DilateSketch(kernel_size=5)]\n",
    "trans = BaseTransform(transforms=transforms)\n",
    "to_image = BaseTransform(normalize=False, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.deepsdf.create_camera(azim=10, elev=-20)\n",
    "with torch.no_grad():\n",
    "    points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "        latent=model.latent,\n",
    "        points=model.deepsdf.camera_points,\n",
    "        mask=model.deepsdf.camera_mask,\n",
    "        rays=model.deepsdf.camera_rays,\n",
    "    )\n",
    "    normals = model.deepsdf.render_normals(\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        mask=surface_mask,\n",
    "    )\n",
    "    # grayscale = model.deepsdf.normals_to_grayscales(normals)\n",
    "    silhouette = model.deepsdf.render_silhouette(\n",
    "        normals=normals,\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        proj_blur_eps=0.7,\n",
    "        weight_blur_kernal_size=9,\n",
    "        weight_blur_sigma=9.0,\n",
    "    )\n",
    "plot_images(\n",
    "    [\n",
    "        normals,\n",
    "        silhouette[\"min_sdf\"],\n",
    "        silhouette[\"base_silhouette\"],\n",
    "        silhouette[\"extra_silhouette\"],\n",
    "        silhouette[\"proj_silhouette\"],\n",
    "        silhouette[\"proj_blur_silhouette\"],\n",
    "        silhouette[\"base_blur_silhouette\"],\n",
    "        silhouette[\"weighted_silhouette\"],\n",
    "        silhouette[\"final_silhouette\"],\n",
    "    ],\n",
    "    size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs = silhouette[\"base_blur_silhouette\"].clone()\n",
    "bbs[bbs > 1e-03] = 0.0\n",
    "ws = silhouette[\"weighted_silhouette\"].clone()\n",
    "bs = silhouette[\"base_silhouette\"].clone()\n",
    "plot_images(torch.clip(bbs + ws + bs, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(silhouette[\"final_silhouette\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silhouette Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 4112, (4117), 12, 13, 4152\n",
    "# obj_label = 4117\n",
    "# obj_label = 4152\n",
    "obj_label = 4112  # [1250,  585, 3916,  673]  4015\n",
    "min_eps = 1e-03\n",
    "max_eps = 5e-02\n",
    "azim = 40\n",
    "elev = -20\n",
    "width = 256\n",
    "height = 256\n",
    "focal = 512\n",
    "\n",
    "# get the sketch form the dataset\n",
    "img = metainfo.load_image(obj_label, 11, 0)\n",
    "sketch = trans(img)[None, ...].to(\"cuda\")\n",
    "\n",
    "# get the rendered normal\n",
    "model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "model.deepsdf.hparams[\"surface_eps\"] = min_eps\n",
    "model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "model.deepsdf.eval()\n",
    "with torch.no_grad():\n",
    "    points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "        latent=model.latent,\n",
    "        points=model.deepsdf.camera_points,\n",
    "        mask=model.deepsdf.camera_mask,\n",
    "        rays=model.deepsdf.camera_rays,\n",
    "    )\n",
    "    image = model.deepsdf.render_grayscale(\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        mask=surface_mask,\n",
    "    )\n",
    "rendered_normal = image.detach().cpu().numpy()\n",
    "sketch, normal = to_image(img).permute(1, 2, 0), rendered_normal\n",
    "\n",
    "normal_input = model.deepsdf.image_to_siamese(torch.tensor(normal).to(\"cuda\"))\n",
    "sketch_input = model.deepsdf.image_to_siamese(torch.tensor(sketch).to(\"cuda\"))\n",
    "normal_emb = model.loss.embedding(normal_input)\n",
    "sketch_emb = model.loss.embedding(sketch_input)\n",
    "loss = model.loss.compute(normal_emb, sketch_emb)\n",
    "print(loss)\n",
    "\n",
    "# silhouette information\n",
    "min_sdf = torch.abs(model.forward(points)).reshape(width, height).detach().cpu()\n",
    "min_points = points.reshape(width, height, 3).detach().cpu()\n",
    "min_normals = ((image - 0.5) / 0.5).detach().cpu()\n",
    "silhouette = ((min_sdf < max_eps) & (min_sdf > min_eps)).numpy().astype(np.uint8)\n",
    "\n",
    "\n",
    "surface_mask = (\n",
    "    surface_mask.reshape(width, height).detach().cpu().numpy().astype(np.uint8)\n",
    ")\n",
    "\n",
    "\n",
    "sil = torch.zeros_like(min_sdf)\n",
    "w2c = model.deepsdf.world_to_camera.detach().cpu().to(torch.float32)\n",
    "idx = np.where(silhouette)\n",
    "w_points = min_points[idx] - min_normals[idx] * min_sdf[idx][..., None]\n",
    "\n",
    "c_points = np.ones((w_points.shape[0], 4))\n",
    "c_points[:, :3] = w_points\n",
    "c_points = w2c @ c_points.T\n",
    "c_points = c_points.T\n",
    "\n",
    "pxs = ((width * 0.5) - (c_points[:, 0] * focal) / c_points[:, 2]).to(torch.int)\n",
    "pys = ((height * 0.5) - (c_points[:, 1] * focal) / c_points[:, 2]).to(torch.int)\n",
    "\n",
    "inside_mask = (pxs >= 0) & (pxs < width) & (pys >= 0) & (pys < height)\n",
    "pxs = pxs[inside_mask]\n",
    "pys = pys[inside_mask]\n",
    "\n",
    "unique_idx, counts = torch.stack([pys, pxs]).unique(dim=1, return_counts=True)\n",
    "sil[unique_idx[0], unique_idx[1]] = counts.to(torch.float32)\n",
    "\n",
    "blur = v2.GaussianBlur(kernel_size=5, sigma=3.0)\n",
    "sil_blur = blur(torch.stack([sil, sil, sil]))[0]\n",
    "sil_blur = torch.clip(sil_blur - 0.5, min=0)\n",
    "\n",
    "blur1 = v2.GaussianBlur(kernel_size=9, sigma=9)\n",
    "in_blur = torch.tensor(\n",
    "    np.stack([surface_mask, surface_mask, surface_mask]), dtype=torch.float32\n",
    ")\n",
    "s_mask = blur1(in_blur)[0]\n",
    "\n",
    "weights = torch.clip(-torch.log(s_mask), 0, 10)\n",
    "final_blur = sil_blur * weights\n",
    "\n",
    "plot_images_np(\n",
    "    [\n",
    "        sketch,\n",
    "        normal,\n",
    "        silhouette,\n",
    "        silhouette + surface_mask * 2,\n",
    "        sil,\n",
    "        sil_blur,\n",
    "        sil_blur + surface_mask * 2,\n",
    "        final_blur,\n",
    "        s_mask,\n",
    "        final_blur + surface_mask * 2,\n",
    "    ],\n",
    "    size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neg Silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10, 4112, (4117), 12, 13, 4152\n",
    "# obj_label = 4117\n",
    "# obj_label = 4152\n",
    "obj_label = 4016\n",
    "min_eps = 1e-06\n",
    "max_eps = 1e-03\n",
    "azim = 30\n",
    "elev = -20\n",
    "width = 256\n",
    "height = 256\n",
    "focal = 512\n",
    "\n",
    "# get the sketch form the dataset\n",
    "img = metainfo.load_sketch(metainfo.obj_ids[obj_label], \"00011\")\n",
    "sketch = trans(img)[None, ...].to(\"cuda\")\n",
    "\n",
    "# get the rendered normal\n",
    "model.latent = model.loss.embedding(sketch, mode=\"sketch\")[0]\n",
    "model.deepsdf.hparams[\"surface_eps\"] = min_eps\n",
    "model.deepsdf.create_camera(azim=azim, elev=elev)\n",
    "model.deepsdf.eval()\n",
    "with torch.no_grad():\n",
    "    points, surface_mask = model.deepsdf.sphere_tracing(\n",
    "        latent=model.latent,\n",
    "        points=model.deepsdf.camera_points,\n",
    "        mask=model.deepsdf.camera_mask,\n",
    "        rays=model.deepsdf.camera_rays,\n",
    "    )\n",
    "    image = model.deepsdf.render_grayscale(\n",
    "        points=points,\n",
    "        latent=model.latent,\n",
    "        mask=surface_mask,\n",
    "    )\n",
    "rendered_normal = image.detach().cpu().numpy()\n",
    "sketch, normal = to_image(img).permute(1, 2, 0), rendered_normal\n",
    "\n",
    "normal_input = model.deepsdf.image_to_siamese(torch.tensor(normal).to(\"cuda\"))\n",
    "sketch_input = model.deepsdf.image_to_siamese(torch.tensor(sketch).to(\"cuda\"))\n",
    "normal_emb = model.loss.embedding(normal_input)\n",
    "sketch_emb = model.loss.embedding(sketch_input)\n",
    "loss = model.loss.compute(normal_emb, sketch_emb)\n",
    "print(loss)\n",
    "\n",
    "# silhouette information\n",
    "min_sdf = torch.abs(model.forward(points)).reshape(width, height).detach().cpu()\n",
    "min_points = points.reshape(width, height, 3).detach().cpu()\n",
    "min_normals = ((image - 0.5) / 0.5).detach().cpu()\n",
    "silhouette = ((min_sdf < max_eps) & (min_sdf > min_eps)).numpy().astype(np.uint8)\n",
    "\n",
    "\n",
    "surface_mask = (\n",
    "    surface_mask.reshape(width, height).detach().cpu().numpy().astype(np.uint8)\n",
    ")\n",
    "\n",
    "\n",
    "sil = torch.zeros_like(min_sdf)\n",
    "w2c = model.deepsdf.world_to_camera.detach().cpu().to(torch.float32)\n",
    "idx = np.where(silhouette)\n",
    "w_points = min_points[idx] - min_normals[idx] * min_sdf[idx][..., None]\n",
    "\n",
    "c_points = np.ones((w_points.shape[0], 4))\n",
    "c_points[:, :3] = w_points\n",
    "c_points = w2c @ c_points.T\n",
    "c_points = c_points.T\n",
    "\n",
    "pxs = ((width * 0.5) - (c_points[:, 0] * focal) / c_points[:, 2]).to(torch.int)\n",
    "pys = ((height * 0.5) - (c_points[:, 1] * focal) / c_points[:, 2]).to(torch.int)\n",
    "\n",
    "inside_mask = (pxs >= 0) & (pxs < width) & (pys >= 0) & (pys < height)\n",
    "pxs = pxs[inside_mask]\n",
    "pys = pys[inside_mask]\n",
    "\n",
    "unique_idx, counts = torch.stack([pys, pxs]).unique(dim=1, return_counts=True)\n",
    "sil[unique_idx[0], unique_idx[1]] = counts.to(torch.float32)\n",
    "\n",
    "blur = v2.GaussianBlur(kernel_size=5, sigma=3.0)\n",
    "sil_blur = blur(torch.stack([sil, sil, sil]))[0]\n",
    "sil_blur = torch.clip(sil_blur - 0.5, min=0)\n",
    "\n",
    "blur1 = v2.GaussianBlur(kernel_size=9, sigma=9)\n",
    "in_blur = torch.tensor(\n",
    "    np.stack([surface_mask, surface_mask, surface_mask]), dtype=torch.float32\n",
    ")\n",
    "s_mask = blur1(in_blur)[0]\n",
    "\n",
    "weights = torch.clip(-torch.log(s_mask), 0, 10)\n",
    "final_blur = sil_blur * weights\n",
    "\n",
    "plot_images(\n",
    "    [\n",
    "        sketch,\n",
    "        normal,\n",
    "        silhouette,\n",
    "        silhouette + surface_mask * 2,\n",
    "        sil,\n",
    "        sil_blur,\n",
    "        sil_blur + surface_mask * 2,\n",
    "        final_blur,\n",
    "        final_blur + surface_mask * 2,\n",
    "    ],\n",
    "    size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 50\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sdf = torch.abs(model.forward(points))\n",
    "silhouette = min_sdf.reshape(256, 256) < model.deepsdf.hparams[\"surface_eps\"] * 10\n",
    "plt.imshow(silhouette.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normals = model.deepsdf.render_normals(points, model.latent, model.deepsdf.camera_mask)\n",
    "plot_images(normals.detach().cpu().numpy())\n",
    "normals = (normals - 0.5) * 2  # IMPORTANT transform back to normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_points = (\n",
    "    points.reshape(256, 256, 3) - normals * min_sdf.reshape(256, 256)[..., None]\n",
    ")\n",
    "(normals * min_sdf.reshape(256, 256)[..., None])[surface_mask.reshape(256, 256)]\n",
    "intrinsic = torch.tensor(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ").to(proj_points)\n",
    "p = proj_points @ model.deepsdf.world_to_camera[:3, :3].T.float()\n",
    "# p = (p @ intrinsic.T)[:, :, :2]\n",
    "\n",
    "# i = np.zeros((256, 256))\n",
    "# for x in tqdm(range(256)):\n",
    "#     for y in range(256):\n",
    "#         px = (p[:, :, 0] >= x) & (p[:, :, 0] <= x+1)\n",
    "#         py = (p[:, :, 1] >= y) & (p[:, :, 1] <= y+1)\n",
    "#         hit = bool((px & py).sum())\n",
    "#         if hit:\n",
    "#             i[x,y] = 1.0\n",
    "\n",
    "from lib.render.camera import Camera\n",
    "\n",
    "camera = Camera()\n",
    "camera.get_camera_to_world() @ np.array([1, 0, 4, 1])\n",
    "intrinsic = np.array(\n",
    "    [\n",
    "        [512, 0, 0],\n",
    "        [0, 512, 0],\n",
    "        [0, 0, 1],\n",
    "    ]\n",
    ")\n",
    "intrinsic @ np.array([0, 0, 4])\n",
    "256 * 0.5 - 128\n",
    "# (0 - 256 * 0.5) / 512\n",
    "# (0 - 256 * 0.5) / 512\n",
    "128 + (512 * 1) / 4, 128 + (512 * 1) / 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
