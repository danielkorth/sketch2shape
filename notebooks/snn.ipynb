{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from lib.data.metainfo import MetaInfo\n",
    "from lib.data.transforms import SketchTransform\n",
    "import hydra\n",
    "from lib.utils import load_config\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torch.nn.functional import l1_loss\n",
    "\n",
    "\n",
    "def stats(normal):\n",
    "    mean = normal.reshape(-1, 3).mean(0)\n",
    "    print(f\"R mean: {mean[0]}\")\n",
    "    print(f\"G mean: {mean[1]}\")\n",
    "    print(f\"B mean: {mean[2]}\")\n",
    "\n",
    "\n",
    "# check why latent is not the right, maybe similar problem\n",
    "def transform(normal):\n",
    "    _transform = v2.Compose(\n",
    "        [\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ]\n",
    "    )\n",
    "    return _transform(normal).to(\"cuda\")\n",
    "\n",
    "\n",
    "def plot_images(images):\n",
    "    if isinstance(images, list):\n",
    "        fig, axes = plt.subplots(1, len(images), figsize=(4, 4))\n",
    "        for ax, image in zip(axes, images):\n",
    "            ax.imshow(image)\n",
    "            ax.axis(\"off\")  # Turn off axis\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(images)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "cfg = load_config(\"traverse_latent\", [\"+experiment/traverse_latent=debug_snn\"])\n",
    "model = hydra.utils.instantiate(cfg.model).to(\"cuda\")\n",
    "metainfo = MetaInfo(\"/home/borth/sketch2shape/data/shapenet_chair_4096\")\n",
    "\n",
    "gt_image_1 = np.asarray(metainfo.load_image(3, 11, 0))\n",
    "gt_image_2 = np.asarray(metainfo.load_image(3, 11, 1))\n",
    "wrong_image_1 = np.asarray(metainfo.load_image(2, 11, 1))\n",
    "\n",
    "model.latent = model.latent_end\n",
    "normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "\n",
    "print(\"sketch\", \"gt_normal\", \"deepsdf_normal\", \"wrong_image_1\")\n",
    "plot_images([gt_image_1, gt_image_2, normal, wrong_image_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN L2-Dist Sketch - Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.latent_end\n",
    "normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "emb_1 = model.siamese(transform(gt_image_1)[None, ...])\n",
    "emb_2 = model.siamese(transform(gt_image_2)[None, ...])\n",
    "print(torch.linalg.vector_norm(emb_1 - emb_2, dim=-1))\n",
    "plot_images([gt_image_1, gt_image_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN L2-Dist Sketch - DeepSDF Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.latent_end\n",
    "normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "emb_1 = model.siamese(transform(gt_image_1)[None, ...])\n",
    "emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "print(torch.linalg.vector_norm(emb_1 - emb_2, dim=-1))\n",
    "plot_images([gt_image_1, normal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN L2-Dist Normal - DeepSDF Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.latent_end\n",
    "normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "emb_1 = model.siamese(transform(gt_image_2)[None, ...])\n",
    "emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "print(torch.linalg.vector_norm(emb_1 - emb_2, dim=-1))\n",
    "plot_images([gt_image_2, normal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN L2-Dist Wrong - DeepSDF Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.latent = model.latent_end\n",
    "normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "emb_1 = model.siamese(transform(wrong_image_1)[None, ...])\n",
    "emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "print(torch.linalg.vector_norm(emb_1 - emb_2, dim=-1))\n",
    "plot_images([wrong_image_1, normal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN L2-Dist GT Normal All Views (90) - DeepSDF Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "l2_dist = []\n",
    "emb_normal = model.siamese(transform(normal)[None, ...])\n",
    "\n",
    "for i in tqdm(range(90), total=90):\n",
    "    gt_image = np.asarray(metainfo.load_image(3, i, 1))\n",
    "    emb_image = model.siamese(transform(gt_image)[None, ...])\n",
    "    _l2_dist = torch.norm(emb_normal - emb_image, dim=-1)\n",
    "    l2_dist.append(_l2_dist)\n",
    "l2_dist = torch.concatenate(l2_dist)\n",
    "print(f\"{l2_dist.mean()=}\")\n",
    "print(f\"{l2_dist.min()=}\")\n",
    "print(f\"{l2_dist.max()=}\")\n",
    "plot_images([gt_image, normal])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15-NN Latent Code Shape(3) Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the top 15 latent codes and then print the SNN loss\n",
    "lats = []\n",
    "for _i in range(10):\n",
    "    latent = model.latent_end\n",
    "    dist = l1_loss(model.model.lat_vecs.weight, latent, reduce=False).mean(-1)\n",
    "    idx = torch.argsort(dist, descending=False)\n",
    "    model.latent = model.model.lat_vecs.weight[idx[_i]]\n",
    "    _n = model.capture_video_frame().detach().cpu().numpy()\n",
    "    lats.append(model.latent)\n",
    "    plot_images(_n)\n",
    "    metainfo.label_to_obj_id(idx[_i].item()), idx[_i], dist[idx[_i]]\n",
    "    l2_dist = []\n",
    "    for i in range(90):\n",
    "        gt_image = np.asarray(metainfo.load_image(idx[_i].item(), i, 0))\n",
    "        emb_image = model.siamese(transform(gt_image)[None, ...])\n",
    "        _l2_dist = torch.norm(emb_normal - emb_image, dim=-1)\n",
    "        l2_dist.append(_l2_dist)\n",
    "    l2_dist = torch.concatenate(l2_dist)\n",
    "    print(idx[_i], l2_dist.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Latent Code from 15-NN Latent Code Shape(3) Prior -> Mean of Latent Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in np.linspace(2, 0, 10):\n",
    "    noise = torch.rand_like(model.latent_end) * torch.stack(lats).std(0) * s\n",
    "    model.latent = torch.stack(lats).mean(0) + noise\n",
    "    normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "    emb_1 = model.siamese(transform(gt_image_1)[None, ...])\n",
    "    emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "    print(torch.norm(emb_1 - emb_2, dim=-1))\n",
    "    plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN Retrieval Top 10 Latent Code Based on Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# encode the images\n",
    "embs = []\n",
    "for i in tqdm(range(4096)):\n",
    "    img = metainfo.load_image(i, 11, 0) # load the normals from a good view\n",
    "    emb = model.siamese(transform(img)[None, ...])\n",
    "    embs.append(emb)\n",
    "embs = torch.stack(embs).squeeze(1)\n",
    "\n",
    "img = metainfo.load_image(3, 11, 0) # load the normals from a good view\n",
    "query_emb= model.siamese(transform(img)[None, ...])\n",
    "\n",
    "# get the top index in SNN space\n",
    "k = 10\n",
    "sx = torch.sum(query_emb**2, dim=-1, keepdim=True)\n",
    "sy = torch.sum(embs**2, dim=-1, keepdim=True)\n",
    "dist = torch.sqrt(-2 * (query_emb @ embs.T) + sx + sy.T)  # (Q, I)\n",
    "dist = dist.nan_to_num(0)\n",
    "idx = torch.argsort(dist)[:, :k]  # (Q, k)\n",
    "top_dist, top_idx = dist.take(idx).detach().cpu().numpy(), idx.detach().cpu().numpy()\n",
    "\n",
    "snn_lats = []\n",
    "for i in top_idx.flatten():\n",
    "    model.latent = model.model.lat_vecs.weight[i]\n",
    "    _n = model.capture_video_frame().detach().cpu().numpy()\n",
    "    snn_lats.append(model.latent)\n",
    "    plot_images(_n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in np.linspace(2, 0, 10):\n",
    "    noise = torch.rand_like(model.latent_end) * torch.stack(snn_lats).std(0) * s\n",
    "    model.latent = torch.stack(snn_lats).mean(0) + noise\n",
    "    normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "    emb_1 = model.siamese(transform(gt_image_1)[None, ...])\n",
    "    emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "    print(torch.norm(emb_1 - emb_2, dim=-1))\n",
    "    plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_id = 4192 # show \n",
    "val_id = 4193\n",
    "val_sketch = metainfo.load_image(val_id, 11, 0) # load the normals from a good view\n",
    "val_normal = metainfo.load_image(val_id, 11, 1) # load the normals from a good view\n",
    "emb_1 = model.siamese(transform(val_sketch)[None, ...])\n",
    "emb_2 = model.siamese(transform(val_normal)[None, ...])\n",
    "print(torch.norm(emb_1 - emb_2, dim=-1))\n",
    "plot_images([val_sketch, val_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_emb= model.siamese(transform(val_sketch)[None, ...])\n",
    "\n",
    "# get the top index in SNN space\n",
    "k = 10\n",
    "sx = torch.sum(query_emb**2, dim=-1, keepdim=True)\n",
    "sy = torch.sum(embs**2, dim=-1, keepdim=True)\n",
    "dist = torch.sqrt(-2 * (query_emb @ embs.T) + sx + sy.T)  # (Q, I)\n",
    "dist = dist.nan_to_num(0)\n",
    "idx = torch.argsort(dist)[:, :k]  # (Q, k)\n",
    "top_dist, top_idx = dist.take(idx).detach().cpu().numpy(), idx.detach().cpu().numpy()\n",
    "\n",
    "val_lats = []\n",
    "val_weights = []\n",
    "for i in top_idx.flatten():\n",
    "    model.latent = model.model.lat_vecs.weight[i]\n",
    "    _n = model.capture_video_frame().detach().cpu().numpy()\n",
    "    val_lats.append(model.latent)\n",
    "    emb_1 = model.siamese(transform(val_sketch)[None, ...])\n",
    "    emb_2 = model.siamese(transform(_n)[None, ...])\n",
    "    weight = torch.norm(emb_1 - emb_2, dim=-1)\n",
    "    val_weights.append(weight)\n",
    "    print(weight)\n",
    "    plot_images(_n) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in np.linspace(2, 0, 10):\n",
    "    noise = torch.rand_like(model.latent_end) * torch.stack(val_lats).std(0) * s\n",
    "    model.latent = torch.stack(val_lats).mean(0) + noise\n",
    "    normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "    emb_1 = model.siamese(transform(gt_image_1)[None, ...])\n",
    "    emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "    print(torch.norm(emb_1 - emb_2, dim=-1))\n",
    "    plot_images(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Latent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20, 2):\n",
    "    w = torch.exp(-i * torch.concatenate(val_weights))\n",
    "    w = w / w.sum()\n",
    "    print(w)\n",
    "    model.latent = (torch.stack(val_lats) * w[..., None]).sum(0)\n",
    "    normal = model.capture_video_frame().detach().cpu().numpy()\n",
    "    emb_1 = model.siamese(transform(val_normal)[None, ...])\n",
    "    emb_2 = model.siamese(transform(normal)[None, ...])\n",
    "    print(torch.norm(emb_1 - emb_2, dim=-1))\n",
    "    plot_images([val_sketch, normal, val_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.scheduler import Coarse2FineScheduler\n",
    "scheduler = Coarse2FineScheduler(resolution=256, milestones=[1, 2])\n",
    "scheduler.current_epoch = 0\n",
    "image = scheduler.downsample(transform(gt_image_2), reducer=\"avg\")\n",
    "image = model.normal_to_image(image, resolution=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sketch2shape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
