{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from lib.visualize.image import visualize_image\n",
    "from lib.render.camera import Camera\n",
    "from lib.models.optimize_latent import LatentOptimizer\n",
    "import torch\n",
    "\n",
    "\n",
    "sphere_eps = 3e-2\n",
    "azim = 0\n",
    "elev = -45\n",
    "dist = 4\n",
    "\n",
    "ckpt_path = \"/home/borth/sketch2shape/checkpoints/deepsdf.ckpt\"\n",
    "camera = Camera(azim=azim, elev=elev, dist=dist, sphere_eps=sphere_eps)\n",
    "points, rays, mask = camera.unit_sphere_intersection_rays()\n",
    "model = LatentOptimizer(\n",
    "    ckpt_path=ckpt_path,\n",
    "    shininess=0.0,\n",
    "    ambient=0.3,\n",
    "    diffuse=0.3,\n",
    "    image_resolution=256,\n",
    "    prior_idx=37,\n",
    "    n_render_steps=100,\n",
    "    surface_eps=2e-03,\n",
    "    step_scale=1,\n",
    "    sphere_eps=sphere_eps,\n",
    ").to(\"cuda:0\")\n",
    "surface_points, surface_mask = model.sphere_tracing(\n",
    "    points=torch.tensor(points, device=model.device),\n",
    "    rays=torch.tensor(rays, device=model.device),\n",
    "    mask=torch.tensor(mask, device=model.device),\n",
    ")\n",
    "normals = model.render_normals(points=surface_points, mask=surface_mask)\n",
    "normal_image = visualize_image(model.normal_to_image(normals, surface_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_points, surface_mask = model.sphere_tracing_min_sdf(\n",
    "    points=points.to(device),\n",
    "    rays=rays.to(device),\n",
    "    mask=mask.to(device),\n",
    ")\n",
    "min_sdf = model.forward(points=min_points)\n",
    "plt.imshow(model.to_image(min_sdf).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = model.to_mesh(resolution=128)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "light_position = torch.tensor([1, 1, 0], dtype=torch.float32, device=device)\n",
    "surface_points, surface_mask = model.sphere_tracing(\n",
    "    points=points.to(device),\n",
    "    rays=rays.to(device),\n",
    "    mask=mask.to(device),\n",
    ")\n",
    "image = model.render_image(\n",
    "    points=surface_points,\n",
    "    mask=surface_mask,\n",
    "    camera_position=camera.camera_position().to(device),\n",
    "    light_position=light_position,\n",
    ")\n",
    "gt_rendered_image = model.to_image(image, surface_mask)\n",
    "_ = visualize_image(gt_rendered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gt_rendered_image.detach().cpu().numpy() * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.data.preprocessing import image_to_sketch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "dist = 4\n",
    "for azim in tqdm(torch.arange(0, 360, 30), total=30):\n",
    "    for elev in torch.arange(0, -60, -15):\n",
    "        camera = Camera(azim=azim, elev=elev, dist=dist, sphere_eps=sphere_eps)\n",
    "        points, rays, mask = camera.unit_sphere_intersection_rays()\n",
    "        light_position = torch.tensor([1, 1, 0], dtype=torch.float32, device=device)\n",
    "        surface_points, surface_mask = model.sphere_tracing(\n",
    "            points=points.to(device),\n",
    "            rays=rays.to(device),\n",
    "            mask=mask.to(device),\n",
    "        )\n",
    "        normals = model.render_normals(points=surface_points, mask=surface_mask)\n",
    "        normal_image = visualize_image(model.normal_to_image(normals, surface_mask))\n",
    "        image = model.render_image(\n",
    "            points=surface_points,\n",
    "            mask=surface_mask,\n",
    "            camera_position=camera.camera_position().to(device),\n",
    "            light_position=light_position,\n",
    "        )\n",
    "        gt_rendered_image = model.to_image(image, surface_mask)\n",
    "        _img = (gt_rendered_image.detach().cpu().numpy() * 255).astype(np.uint8)\n",
    "        _sketch = image_to_sketch(\n",
    "            [_img], t_lower=60, t_upper=100, aperture_size=3, L2gradient=True\n",
    "        )[0]\n",
    "        cv2.imwrite(f\"temp/sketches/{azim}-{-elev}-{4}-sketch.png\", _sketch)\n",
    "        cv2.imwrite(\n",
    "            f\"temp/images/{azim}-{-elev}-{4}-image.png\",\n",
    "            cv2.cvtColor(\n",
    "                gt_rendered_image.detach().cpu().numpy() * 255, cv2.COLOR_RGB2BGR\n",
    "            ),\n",
    "        )\n",
    "        cv2.imwrite(\n",
    "            f\"temp/normals/{azim}-{-elev}-{4}-normal.png\",\n",
    "            cv2.cvtColor(normal_image * 255, cv2.COLOR_RGB2BGR),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import normalize\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"/home/borth/sketch2shape/data/siamese_chair_large\")\n",
    "\n",
    "\n",
    "def read_image(path: Path, device: str):\n",
    "    transform = transforms.ToTensor()\n",
    "    return transform(cv2.imread(str(path))).to(device)\n",
    "\n",
    "\n",
    "gt_sketch_path = Path(path, \"1a6f615e8b1b5ae4dbbc9440457e303e/sketches/00021.jpg\")\n",
    "gt_sketch = read_image(gt_sketch_path, device=device)\n",
    "plt.imshow(gt_sketch.permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "gt_rendered_image_path = Path(path, \"fc4d15c15c56aa7baab4888e25356418/images/00021.jpg\")\n",
    "gt_rendered_image = read_image(gt_rendered_image_path, device=device)\n",
    "plt.imshow(gt_rendered_image.permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torch.optim import Adam\n",
    "from torch.nn.functional import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "sketch = gt_sketch.clone().detach()\n",
    "rendered_image = gt_rendered_image[0].clone().detach()\n",
    "rendered_image.requires_grad = True\n",
    "# rendered_image *= torch.randn_like(rendered_image)\n",
    "\n",
    "optimizer = Adam([rendered_image], lr=1e-03)\n",
    "\n",
    "for step in (pbar := tqdm(range(100))):\n",
    "    optimizer.zero_grad()\n",
    "    output = siamese.forward(\n",
    "        {\"sketch\": sketch[None], \"image\": rendered_image.expand(3, 256, 256)[None]}\n",
    "    )\n",
    "    sketch_emb = normalize(output[\"sketch_emb\"], dim=-1)\n",
    "    image_emb = normalize(output[\"image_emb\"], dim=-1)\n",
    "    emb_loss = -(sketch_emb @ image_emb.T).sum() + 1\n",
    "    # color_loss = torch.norm(rendered_image - rendered_image.mean(dim=0), p=1) * 0.1\n",
    "\n",
    "    # loss = color_loss + emb_loss\n",
    "    loss = emb_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # ensures that the image stays in correct values\n",
    "    with torch.no_grad():\n",
    "        for param in optimizer.param_groups[0][\"params\"]:\n",
    "            param.clamp_(0, 1)\n",
    "\n",
    "    pbar.set_postfix_str(f\"{loss=:.3f}\")\n",
    "    # pbar.set_postfix_str(f\"{loss=:.3f}, {emb_loss=:.3f}, {color_loss=:.3f}\")\n",
    "    # if step % 100 == 0:\n",
    "    #     plt.imshow(rendered_image.permute(1, 2, 0).detach().cpu().numpy())\n",
    "    #     plt.show()\n",
    "\n",
    "plt.imshow(gt_sketch.permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(gt_rendered_image.permute(1, 2, 0).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_image = gt_rendered_image[0].clone().detach()\n",
    "rendered_image.requires_grad = True\n",
    "\n",
    "sketch = gt_sketch[0].clone().detach()\n",
    "sketch.requires_grad = True\n",
    "\n",
    "output = siamese.forward(\n",
    "    {\n",
    "        \"sketch\": sketch.expand(3, 256, 256)[None],\n",
    "        \"image\": rendered_image.expand(3, 256, 256)[None],\n",
    "    }\n",
    ")\n",
    "# sketch_emb = normalize(output[\"sketch_emb\"], dim=-1)\n",
    "# image_emb = normalize(output[\"image_emb\"], dim=-1)\n",
    "emb_loss = -(output[\"sketch_emb\"] @ output[\"image_emb\"].T).sum()\n",
    "loss = emb_loss\n",
    "\n",
    "grad_image = torch.autograd.grad(\n",
    "    outputs=loss,\n",
    "    inputs=rendered_image,\n",
    "    grad_outputs=torch.ones_like(loss),\n",
    "    retain_graph=True,\n",
    ")[0]\n",
    "\n",
    "grad_sketch = torch.autograd.grad(\n",
    "    outputs=loss,\n",
    "    inputs=sketch,\n",
    "    grad_outputs=torch.ones_like(loss),\n",
    ")[0]\n",
    "\n",
    "plt.imshow(grad_image.detach().cpu().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(grad_sketch.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface_mask = torch.ones_like(rendered_image)\n",
    "surface_mask[rendered_image > 0.95] = 0\n",
    "plt.imshow(surface_mask.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_grad_image = grad_image.clone()\n",
    "plt.imshow(_grad_image.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_map = grad_image - grad_sketch\n",
    "img = torch.ones_like(grad_sketch)\n",
    "# img[] = 0\n",
    "plt.imshow(grad_map.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.ones_like(grad_sketch)\n",
    "img[torch.abs(grad_sketch) > 0.1] = 0\n",
    "plt.imshow(img.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.ones_like(grad_image)\n",
    "img[grad_image < -0.2] = 0\n",
    "plt.imshow(img.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.ones_like(grad_image)\n",
    "img[grad_image > 0.2] = 0\n",
    "plt.imshow(img.detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
