# @package _global_

defaults:
  - override /data:
    - shapenet_chair_4096
    - train_siamese
    - eval_siamese
  - override /logger: wandb

tags: ["train_siamese", "shapenet_chair_4096"]

trainer:
  max_epochs: 500 
  check_val_every_n_epoch: 10

model:
  norm: 2
  reg_weight: 1e-01
  lr_head: 1e-03
  lr_backbone: 1e-04
  embedding_size: 128
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    _partial_: True
    step_size: 100
    gamma: 0.5

data:
  batch_size: 256
  
callbacks:
  model_checkpoint:
    save_top_k: -1
    every_n_epochs: 100
    save_last: True
  early_stopping:
    monitor: val/loss
    patience: 2000 
