# @package _global_

defaults:
  - override /data: siamese_chair_large
  - override /model: siamese
  - override /callbacks: default
  - override /trainer: default
  - override /logger: wandb

task_name: "train"
test: True

trainer:
  accelerator: gpu
  max_epochs: 10
  val_check_interval: 200
  log_every_n_steps: 1

data:
  batch_size: 128
  num_workers: 7
  pin_memory: True
  drop_last: True
  persistent_workers: True
  shuffle: False
  sampler:
    _target_: pytorch_metric_learning.samplers.MPerClassSampler
    _partial_: True
    m: 2
  dataset:
    _target_: lib.data.siamese_dataset.SiameseDatasetDynamicLoadDynamicTransform
  collate_fn: null

model:
  scheduler: null
  optimizer:
    lr: 1e-03
  decoder:
    _target_: lib.models.decoder.ResNet18
    embedding_size: 128
  miner:
    _target_: pytorch_metric_learning.miners.TripletMarginMiner
    margin: 0.2
    type_of_triplets: semihard
  loss:
    _target_: pytorch_metric_learning.losses.TripletMarginLoss
    margin: 0.2
    swap: False
    smooth_loss: False
    triplets_per_anchor: all
    reducer: 
      _target_: pytorch_metric_learning.reducers.ThresholdReducer
      low: 0
  mine_full_batch: true
  scale_loss: true
  
callbacks:
  early_stopping:
    monitor: "val/loss"
    patience: 8


tags: ["siamese", "multirun", "mine_full_batch"]

logger:
  wandb:
    tags: ${tags}
    group: "siamese"