# @package _global_

defaults:
  - override /data:
    - train_siamese
    - eval_siamese
  - override /dataset: shapenet_chair_4096
  - override /logger: wandb
  - override /model: siamese

task_name: train_siamese
tags: ["train_siamese", "shapenet_chair_4096"]

trainer:
  max_epochs: 2
  check_val_every_n_epoch: 1

model:
  reg_weight: 1e-01
  lr_head: 1e-03
  lr_backbone: 1e-04
  embedding_size: 128
  norm: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    _partial_: True
    step_size: 100
    gamma: 0.5

data:
  batch_size: 256 
  
callbacks:
  # model_checkpoint:
  #   save_top_k: -1
  #   every_n_epochs: 1
  #   save_last: True
  early_stopping:
    monitor: val/loss
    patience: 2000 
