# @package _global_

defaults:
  - override /data:
    - train_loss
    - eval_loss
  - override /dataset: shapenet_chair_4096
  - override /logger: wandb
  - override /model: siamese

task_name: train_siamese
tags: ["train_loss", "shapenet_chair_4096", "siamese"]

trainer:
  max_epochs: 500
  check_val_every_n_epoch: 10

model:
  reg_weight: 1e-01
  lr_head: 1e-03
  lr_backbone: 1e-04
  embedding_size: 128
  norm: 2
  scheduler:
    _target_: torch.optim.lr_scheduler.StepLR
    _partial_: True
    step_size: 100
    gamma: 0.5

data:
  batch_size: 256 
  
callbacks:
  model_checkpoint:
    save_top_k: -1
    every_n_epochs: 100
    save_last: True
  early_stopping:
    monitor: val/loss
    patience: 2000 
