# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: deepsdf_chair
  - override /model: deepsdf
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["overfit_batch", "our_machine"]

trainer:
  accelerator: gpu
  max_epochs: 2000
  precision: 16-mixed
  log_every_n_steps: 10

data:
  experiment: train
  data_dir: ${paths.deepsdf_dir}/SdfSamples/${data.experiment}
  norm_dir: ${paths.deepsdf_dir}/NormalizationParameters/${data.experiment}
  surf_dir: ${paths.deepsdf_dir}/SurfaceSamples/${data.experiment}
  batch_size: 24
  num_workers: 4
  pin_memory: False
  drop_last: True
  shuffle: True
  persistent_workers: True
  subsample: 16384

model:
  latent_size: 512
  num_hidden_layers: 8
  latent_vector_size: 256
  clamp: True
  clamp_val: 0.1
  reg_loss: True
  num_scenes: 3282
  sigma: 1e-4
  skip_connection: [4]
  dropout: True
  dropout_p: 0.2
  weight_norm: True

  decoder_scheduler:
    _partial_: True
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 100
    gamma: 0.9

  latents_scheduler:
    _partial_: True
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 100
    gamma: 0.9

callbacks:
  model_checkpoint:
    every_n_epochs: 100
    save_last: True
    save_top_k: -1

logger:
  wandb:
    tags: ${tags}
    group: "deepsdf"
    # name: $data.experiment-bs_{data.batch_size}
