# @package _global_

defaults:
  - data:
    - optimize_latent 
  - model: optimize_latent
  - trainer: default
  - logger: default
  - hydra: default
  - paths: default
  - debug: null
  - optional local: default
  - _self_

seed: 123
task_name: optimize_deepsdf
tags: ["optimize_deepsdf"]
split: val  # train, val, test
obj_ids: ???  # overrids the split setting, make sure that obj_ids are only from the same split.
train: True
prior_idx: mean  # random(-2), mean(-1), prior, prior(idx)
eval: True
save_mesh: True  # only if eval == True
create_video: True 
ckpt_path: ${paths.checkpoint_dir}/deepsdf.ckpt

trainer:
  num_sanity_val_steps: 0
  max_epochs: 2000 

data:
  train_dataset:
    _target_: lib.data.optimize_latent_dataset.DeepSDFLatentOptimizerDataset
    chunk_size: 16384 
    half: True

model:
  _target_: lib.models.optimize_deepsdf.DeepSDFLatentOptimizer
  adaptive_sample_strategy: True
  adaptive_mining_strategy: True
  reg_loss: True
  reg_weight: 1e-04
  latents_lr: 1e-03
  capture_rate: 30
  scheduler:
    _partial_: True
    _target_: torch.optim.lr_scheduler.StepLR
    step_size: 500
    gamma: 0.5
  
callbacks:
  early_stopping:
    patience: 2000